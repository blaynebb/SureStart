{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3gelNmw0QeG"
      },
      "source": [
        "# Basic packages\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import re\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "# Packages for data preparation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Packages for modeling\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
        "NB_START_EPOCHS = 20  # Number of epochs we usually start to train with\n",
        "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
        "MAX_LEN = 20  # Maximum number of words in a sequence\n",
        "root = Path('../')\n",
        "input_path = root / 'input/' \n",
        "ouput_path = root / 'output/'\n",
        "source_path = root / 'source/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp512gcB0Vas"
      },
      "source": [
        "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
        "    '''\n",
        "    Function to train a multi-class model. The number of epochs and \n",
        "    batch_size are set by the constants at the top of the\n",
        "    notebook. \n",
        "    \n",
        "    Parameters:\n",
        "        model : model with the chosen architecture\n",
        "        X_train : training features\n",
        "        y_train : training target\n",
        "        X_valid : validation features\n",
        "        Y_valid : validation target\n",
        "    Output:\n",
        "        model training history\n",
        "    '''\n",
        "    model.compile(optimizer='rmsprop'\n",
        "                  , loss='categorical_crossentropy'\n",
        "                  , metrics=['accuracy'])\n",
        "    \n",
        "    history = model.fit(X_train\n",
        "                       , y_train\n",
        "                       , epochs=NB_START_EPOCHS\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=0)\n",
        "    return history\n",
        "def eval_metric(model, history, metric_name):\n",
        "    '''\n",
        "    Function to evaluate a trained model on a chosen metric. \n",
        "    Training and validation metric are plotted in a\n",
        "    line chart for each epoch.\n",
        "    \n",
        "    Parameters:\n",
        "        history : model training history\n",
        "        metric_name : loss or accuracy\n",
        "    Output:\n",
        "        line chart with epochs of x-axis and metric on\n",
        "        y-axis\n",
        "    '''\n",
        "    metric = history.history[metric_name]\n",
        "    val_metric = history.history['val_' + metric_name]\n",
        "    e = range(1, NB_START_EPOCHS + 1)\n",
        "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
        "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
        "    plt.xlabel('Epoch number')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.title('Comparing training and validation ' + metric_name + ' for ' + model.name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
        "    '''\n",
        "    Function to test the model on new data after training it\n",
        "    on the full training data with the optimal number of epochs.\n",
        "    \n",
        "    Parameters:\n",
        "        model : trained model\n",
        "        X_train : training features\n",
        "        y_train : training target\n",
        "        X_test : test features\n",
        "        y_test : test target\n",
        "        epochs : optimal number of epochs\n",
        "    Output:\n",
        "        test accuracy and test loss\n",
        "    '''\n",
        "    model.fit(X_train\n",
        "              , y_train\n",
        "              , epochs=epoch_stop\n",
        "              , batch_size=BATCH_SIZE\n",
        "              , verbose=0)\n",
        "    results = model.evaluate(X_test, y_test)\n",
        "    print()\n",
        "    print('Test accuracy: {0:.2f}%'.format(results[1]*100))\n",
        "    return results\n",
        "    \n",
        "def remove_stopwords(input_text):\n",
        "    '''\n",
        "    Function to remove English stopwords from a Pandas Series.\n",
        "    \n",
        "    Parameters:\n",
        "        input_text : text to clean\n",
        "    Output:\n",
        "        cleaned Pandas Series \n",
        "    '''\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "    whitelist = [\"n't\", \"not\", \"no\"]\n",
        "    words = input_text.split() \n",
        "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "    return \" \".join(clean_words) \n",
        "    \n",
        "def remove_mentions(input_text):\n",
        "    '''\n",
        "    Function to remove mentions, preceded by @, in a Pandas Series\n",
        "    \n",
        "    Parameters:\n",
        "        input_text : text to clean\n",
        "    Output:\n",
        "        cleaned Pandas Series \n",
        "    '''\n",
        "    return re.sub(r'@\\w+', '', input_text)\n",
        "def compare_models_by_metric(model_1, model_2, model_hist_1, model_hist_2, metric):\n",
        "    '''\n",
        "    Function to compare a metric between two models \n",
        "    \n",
        "    Parameters:\n",
        "        model_hist_1 : training history of model 1\n",
        "        model_hist_2 : training history of model 2\n",
        "        metrix : metric to compare, loss, acc, val_loss or val_acc\n",
        "        \n",
        "    Output:\n",
        "        plot of metrics of both models\n",
        "    '''\n",
        "    metric_model_1 = model_hist_1.history[metric]\n",
        "    metric_model_2 = model_hist_2.history[metric]\n",
        "    e = range(1, NB_START_EPOCHS + 1)\n",
        "    \n",
        "    metrics_dict = {\n",
        "        'acc' : 'Training Accuracy',\n",
        "        'loss' : 'Training Loss',\n",
        "        'val_acc' : 'Validation accuracy',\n",
        "        'val_loss' : 'Validation loss'\n",
        "    }\n",
        "    \n",
        "    metric_label = metrics_dict[metric]\n",
        "    plt.plot(e, metric_model_1, 'bo', label=model_1.name)\n",
        "    plt.plot(e, metric_model_2, 'b', label=model_2.name)\n",
        "    plt.xlabel('Epoch number')\n",
        "    plt.ylabel(metric_label)\n",
        "    plt.title('Comparing ' + metric_label + ' between models')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "def optimal_epoch(model_hist):\n",
        "    '''\n",
        "    Function to return the epoch number where the validation loss is\n",
        "    at its minimum\n",
        "    \n",
        "    Parameters:\n",
        "        model_hist : training history of model\n",
        "    Output:\n",
        "        epoch number with minimum validation loss\n",
        "    '''\n",
        "    min_epoch = np.argmin(model_hist.history['val_loss']) + 1\n",
        "    print(\"Minimum validation loss reached in epoch {}\".format(min_epoch))\n",
        "    return min_epoch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "wT4UJRAo0bG5",
        "outputId": "ff75e50c-2b28-483e-ad12-cdda695c4de4"
      },
      "source": [
        "df = pd.read_csv(input_path / 'Tweets.csv')\n",
        "df = df.reindex(np.random.permutation(df.index))  \n",
        "df = df[['text', 'airline_sentiment']]\n",
        "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-adf2a1ae57bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'Tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'airline_sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_mentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/Tweets.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCpPAEzI0eIe"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LkbP7Ek0nSr"
      },
      "source": [
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{\"}~\\t\\n',\n",
        "               lower=True,\n",
        "               char_level=False,\n",
        "               split=' ')\n",
        "tk.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT7INTIF0qCD"
      },
      "source": [
        "X_train_oh = tk.texts_to_matrix(X_train, mode='binary')\n",
        "X_test_oh = tk.texts_to_matrix(X_test, mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUxqzN8h0tjW"
      },
      "source": [
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_test_le = le.transform(y_test)\n",
        "y_train_oh = to_categorical(y_train_le)\n",
        "y_test_oh = to_categorical(y_test_le)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVAqeJDf0v2m"
      },
      "source": [
        "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PZwNszR00os"
      },
      "source": [
        "base_model = models.Sequential()\n",
        "base_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
        "base_model.add(layers.Dense(64, activation='relu'))\n",
        "base_model.add(layers.Dense(3, activation='softmax'))\n",
        "base_model.name = 'Baseline model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC0BKViE03Rp"
      },
      "source": [
        "base_history = deep_model(base_model, X_train_rest, y_train_rest, X_valid, y_valid)\n",
        "base_min = optimal_epoch(base_history)\n",
        "eval_metric(base_model, base_history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtaQ1Gcw07Hd"
      },
      "source": [
        "reduced_model = models.Sequential()\n",
        "reduced_model.add(layers.Dense(16, activation='relu', input_shape=(NB_WORDS,)))\n",
        "reduced_model.add(layers.Dense(3, activation='softmax'))\n",
        "reduced_model.name = 'Reduced model'\n",
        "reduced_history = deep_model(reduced_model, X_train_rest, y_train_rest, X_valid, y_valid)\n",
        "reduced_min = optimal_epoch(reduced_history)\n",
        "eval_metric(reduced_model, reduced_history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHcNx7JI09f8"
      },
      "source": [
        "compare_models_by_metric(base_model, reduced_model, base_history, reduced_history, 'val_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p79qc9yU0_3i"
      },
      "source": [
        "reg_model = models.Sequential()\n",
        "reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(NB_WORDS,)))\n",
        "reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
        "reg_model.add(layers.Dense(3, activation='softmax'))\n",
        "reg_model.name = 'L2 Regularization model'\n",
        "reg_history = deep_model(reg_model, X_train_rest, y_train_rest, X_valid, y_valid)\n",
        "reg_min = optimal_epoch(reg_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQA1L3Y21AmO"
      },
      "source": [
        "eval_metric(reg_model, reg_history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im8RzWDT1EoK"
      },
      "source": [
        "compare_models_by_metric(base_model, reg_model, base_history, reg_history, 'val_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJfNH5oh1KPx"
      },
      "source": [
        "drop_model = models.Sequential()\n",
        "drop_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
        "drop_model.add(layers.Dropout(0.5))\n",
        "drop_model.add(layers.Dense(64, activation='relu'))\n",
        "drop_model.add(layers.Dropout(0.5))\n",
        "drop_model.add(layers.Dense(3, activation='softmax'))\n",
        "drop_model.name = 'Dropout layers model'\n",
        "drop_history = deep_model(drop_model, X_train_rest, y_train_rest, X_valid, y_valid)\n",
        "drop_min = optimal_epoch(drop_history)\n",
        "eval_metric(drop_model, drop_history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGH8qnNh1NCg"
      },
      "source": [
        "compare_models_by_metric(base_model, drop_model, base_history, drop_history, 'val_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vu_zMN-1PpJ"
      },
      "source": [
        "base_results = test_model(base_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, base_min)\n",
        "reduced_results = test_model(reduced_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, reduced_min)\n",
        "reg_results = test_model(reg_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, reg_min)\n",
        "drop_results = test_model(drop_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, drop_min)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}